https://youtu.be/FoKMxugH4hI?si=ahd2_Mrx_76k8PR9&t=190

# Cloud Storage (i.e. AWS)
3 types
1. **==object store==** (AWS S3, GCP cloud storage, Dropbox, backups)
	- why they use chain replication?  large objects. 
	- tons of unstructured data, flat (unlike hierarchy in normal file sys)
		- simulate directory with "prefix" to filename in bucket, some clients recognize as folder
	- for **data lakes** diverse data, long term data storage (archival logs, static assets for website)
	- not attached to single server or disk, standalone, flexible, for AWS logs
	- each file organized in **buckets** w/ unique name (across ALL of AWS not region)
	- buckets replicated **among** regions and **within** regions
		- among: spread across multiple regions
		- within: availability zone in case of geographic outage/locality in data center 
	- tradeoff/tiers
		- high perf low latency \$\$\$\$ (expensive to store)
		- infrequently accessed (move inactive users, more expensive to retrieval)
		- archival (rarely used data, dirt cheap, \$\$\$\$ retrieve i.e. in cave super slow)
		- [[db8806d4c933e772e268c6c83fdd46cd_MD5.jpeg|Open: Pasted image 20241123000036.png]]![[db8806d4c933e772e268c6c83fdd46cd_MD5.jpeg]]
	- nickel-diming pricing (always price to retrieve/egress data, free upload/ingress)
		- region (LA \$\$\$)
		- data (GBs/month, discounts)
		- num requests (every 1k PUT/COPY/POST/WRITE,  GET/SELECT)
		- storage class changes (every 1k) - auto intelligient tiers also \$\$\$
		- replication
		- Examples
			- store 50GB in standard, 100k files. Egress=500GB (i.e. retrieve 10 times, 1 million reads)
			- so: writes=100k writes. storage: 50GB in standard, standard pricing=.023/GB and .005/1k writes
			- ingress: 100k writes / 1k x $.005 + 50 GB(\$.023) = 1.65
			- egress: $.09(500GB) + 1M reads / 1k x \$.0004 = 45.4
			- total: $1.65+$45.4 = $47.05
		- lots of small files but total lots of GB = tons of writes, reads, \$\$\$\$
3. **==block lvl==** store (AWS EBS, GCP persistent disk)
	- like disk in computer, attached to server instance, needs mounting, partition, formatting
	- way faster than object storage like real SSD
	- pricing: EC2 server runtime + storing/reading OS installation into the EBS
	- classes w/ sub-versions (eras)
		- general purpose (SSD), practical, cost effective, IOPS not guaranteed
		- provisioned IOPS (faster, IOPS=I/Os per sec) - databases, production web servers
		- throughput optimized hdd - optimized for high thruput i.e. WD Purple
		- cold hdd = commodity hdd slow, cheap
	- pricing - even if unmounted or not associated with instance
		- region
		- storage class
		- num provisioned IOPS above baseline
		- thruput MB/s
		- i.e. small files = greater thruput and IOPS
4. **==cloud/distrib. file sys==** (AWS EFS, GCP Filestore)
	- like EBS is disk needing mounting, but supports multiple EC2 server instance connections not just one
	- ideally EC2 server and EBS disk in same data center
	- unlike SSD partitions + file sys, EFS model completely independent of disks
		- simply shared file system abstract away underlying disk vols
	- classes
		- tiering = automatic placement/optimizing of which tier, decided per file
		[[e19e6383cf5eab3dcfc205d8931aad0e_MD5.jpeg|Open: Pasted image 20241123003030.png]]
![[e19e6383cf5eab3dcfc205d8931aad0e_MD5.jpeg]]
# Consensus
replication and coordination failures
- GFS: master failure fine just heartbeat chunkservers for what they contain + operation log snapshot for location of chunkservers
- chain replication: head(write coordinator) and tail (read coord)
	- middle node dies: remove from linked list
	- head node dies: make next node w/ already replicated data new head (easy) or elect new head
- 
# Paxos
https://youtu.be/FoKMxugH4hI?si=ahd2_Mrx_76k8PR9&t=190

# Cloud Storage (i.e. AWS)
3 types
1. **==object store==** (AWS S3, GCP cloud storage)
	- tons of unstructured data, flat (unlike hierarchy in normal file sys)
		- simulate directory with "prefix" to filename in bucket, some clients recognize as folder
	- for **data lakes** diverse data, long term data storage (archival logs, static assets for website) 
	- not attached to single server or disk
	- each file organized in **buckets** w/ unique name (across ALL of AWS not region)
	- buckets replicated **among** regions and **within** regions
		- among: spread across multiple regions
		- within: availability zone in case of geographic outage/locality in data center 
	- tradeoff/tiers
		- high perf low latency \$\$\$\$ (expensive to store)
		- infrequently accessed (move inactive users, more expensive to retrieval)
		- archival (rarely used data, dirt cheap, \$\$\$\$ retrieve i.e. in cave super slow)
		- [[db8806d4c933e772e268c6c83fdd46cd_MD5.jpeg|Open: Pasted image 20241123000036.png]]![[db8806d4c933e772e268c6c83fdd46cd_MD5.jpeg]]
	- nickel-diming pricing (always price to retrieve/egress data, free upload/ingress)
		- region (LA \$\$\$)
		- data (GBs/month, discounts)
		- num requests (every 1k PUT/COPY/POST/WRITE,  GET/SELECT)
		- storage class changes (every 1k) - auto intelligient tiers also \$\$\$
		- replication
		- Examples
			- store 50GB in standard, 100k files. Egress=500GB (i.e. retrieve 10 times, 1 million reads)
			- so: writes=100k writes. storage: 50GB in standard, standard pricing=.023/GB and .005/1k writes
			- ingress: 100k writes / 1k x $.005 + 50 GB(\$.023) = 1.65
			- egress: $.09(500GB) + 1M reads / 1k x \$.0004 = 45.4
			- total: $1.65+$45.4 = $47.05
1. block lvl store (AWS EBS, GCP persistent disk)
2. distrib. file sys (AWS EFS, GCP Filestore)

# Consensus

# Paxos
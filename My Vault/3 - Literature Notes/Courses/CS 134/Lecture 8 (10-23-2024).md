https://youtu.be/qpfmF4g24Oo?si=NJk30Ar8mXGXkzTR&t=1546

- consistency models:
	- network (async/sync)
	- fault/failure (RPC)
	- ordering models (causal)
	- delivery models (crash, omission)
- **TOTAL ORDER DELIVERY**
	- if m1 delivered before m2 on ONE mode, same for ALL nodes
	- only a property of ONE run, 
	- not enough - need **DETERMINISM** (every run same)
		- here: only 1 replica R so guarantees total order, but two possibilities not deterministic
		- [[b89c8100f38c27da99b4abd331e3532f_MD5.jpeg|Open: Pasted image 20241122111438.png]]
	- 
![[b89c8100f38c27da99b4abd331e3532f_MD5.jpeg]]
- consistency violations among replicas
	- ==Eventual consistency==: one of weakest
		- "stale" data? device showing old msg as speed > consistency (more important), eventually see update (common in NoSQL, big data stores/sys, etc.)
		- i.e. ML reading slightly outdated weights to speed up training
	- ==Read your writes==: writes to R1, reads from R2 (either load balance or R1 crash)
		- client writes 5, gets ACK, reads a 4?!??
		- solution: R1 replicates new written value before ACKing to client (**sync. replication**)
	- ==FIFO consistency==: client deposits $50 to R1, gets ACK, R1
		- R2 gets in wrong order
		- [[948c62f0a4eabf003a5d6d8f02ed63ab_MD5.jpeg|Open: Pasted image 20241122112025.png]]
![[948c62f0a4eabf003a5d6d8f02ed63ab_MD5.jpeg]]
	- ==Causal consistency==: know deposit -->withdrawal (happens-before relation) but other replication doedsn't see this relation
		- soln: vector clocks, causal broadcast, see prev. lecture
		- [[6aa5ff5fabda8928728da5f1df7ee6bb_MD5.jpeg|Open: Pasted image 20241122112421.png]]
![[6aa5ff5fabda8928728da5f1df7ee6bb_MD5.jpeg]]
- [[aa3be14964dc07fdf7d886e7234c429b_MD5.jpeg|Open: Pasted image 20241122112531.png]]
![[aa3be14964dc07fdf7d886e7234c429b_MD5.jpeg]]
	- causal is usually good enough in practice
	- strong: \$\$\$\$, network I/O and time to ensure eplication + ACKs
	- ==**CAP theorem** (Consistency, Availability, and Partition Tolerance)==
		- distrib. system can only support two of the three desired properties
		- partition tolerance: system runs even if partitioned (aka each group of nodes cannot talk to the other)
	-  if partition failure: sacrifice either availiability (fast) or consistency (right), otherwise prioritize one or the other 
		- i.e. partition failure + availability: give stale answers
		- i.e. partition failure + consistency: smaller partition rejects all requests until back online
## GFS:
- prioritize bandwidth storing and accessing of massive data
- distributed across tons of low commodity old hard disk hardware
- expose simple API to users (abstracting away physical location to user)
- features
	- **performance** - throughput >> latency (i.e. mapreduce set and forget)
	- **scalability** - easily add/drop nodes automatically
	- **reliability** - more-or-less consistent data (looser consistency model)
	- **availability** - high uptime (NOT CAP's def. of always get response)
- hard disk -> tracks -> sectors -> blocks of data
	- want large files sequential 
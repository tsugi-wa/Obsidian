https://youtu.be/qpfmF4g24Oo?si=NJk30Ar8mXGXkzTR&t=1546

- consistency models:
	- network (async/sync)
	- fault/failure (RPC)
	- ordering models (causal)
	- delivery models (crash, omission)
- **TOTAL ORDER DELIVERY**
	- if m1 delivered before m2 on ONE mode, same for ALL nodes
	- only a property of ONE run, 
	- not enough - need **DETERMINISM** (every run same)
		- here: only 1 replica R so guarantees total order, but two possibilities not deterministic
		- [[b89c8100f38c27da99b4abd331e3532f_MD5.jpeg|Open: Pasted image 20241122111438.png]]
	- 
![[b89c8100f38c27da99b4abd331e3532f_MD5.jpeg]]
- consistency violations among replicas
	- ==Eventual consistency==: one of weakest
		- "stale" data? device showing old msg as speed > consistency (more important), eventually see update (common in NoSQL, big data stores/sys, etc.)
		- i.e. ML reading slightly outdated weights to speed up training
	- ==Read your writes==: writes to R1, reads from R2 (either load balance or R1 crash)
		- client writes 5, gets ACK, reads a 4?!??
		- solution: R1 replicates new written value before ACKing to client (**sync. replication**)
	- ==FIFO consistency==: client deposits $50 to R1, gets ACK, R1
		- R2 gets in wrong order
		- [[948c62f0a4eabf003a5d6d8f02ed63ab_MD5.jpeg|Open: Pasted image 20241122112025.png]]
![[948c62f0a4eabf003a5d6d8f02ed63ab_MD5.jpeg]]
	- ==Causal consistency==: know deposit -->withdrawal (happens-before relation) but other replication doedsn't see this relation
		- soln: vector clocks, causal broadcast, see prev. lecture
		- [[6aa5ff5fabda8928728da5f1df7ee6bb_MD5.jpeg|Open: Pasted image 20241122112421.png]]
![[6aa5ff5fabda8928728da5f1df7ee6bb_MD5.jpeg]]
- [[aa3be14964dc07fdf7d886e7234c429b_MD5.jpeg|Open: Pasted image 20241122112531.png]]
![[aa3be14964dc07fdf7d886e7234c429b_MD5.jpeg]]
	- causal is usually good enough in practice
	- strong: \$\$\$\$, network I/O and time to ensure eplication + ACKs
	- ==**CAP theorem** (Consistency, Availability, and Partition Tolerance)==
		- distrib. system can only support two of the three desired properties
		- partition tolerance: system runs even if partitioned (aka each group of nodes cannot talk to the other)
	-  if partition failure: sacrifice either availiability (fast) or consistency (right), otherwise prioritize one or the other 
		- i.e. partition failure + availability: give stale answers
		- i.e. partition failure + consistency: smaller partition rejects all requests until back online
## GFS:
- prioritize bandwidth storing and accessing of massive data
- distributed across tons of low commodity old hard disk hardware
- expose simple API to users (abstracting away physical location to user)
- features
	- **performance** - throughput >> latency (i.e. mapreduce set and forget)
	- **scalability** - easily add/drop nodes automatically
	- **reliability** - more-or-less consistent data (looser consistency model)
	- **availability** - high uptime (NOT CAP's def. of always get response)
- hard disk -> tracks -> sectors -> blocks of data
	- want large files stored contiguous/sequential, why? faster:
		- seek time + # blocks read + time to transfer block < random access
	- small files imply random access, less well supported but is. 
- two modes of reading & writing
	- large stream read, large seq. appends (optimized)
	- small random read/writes (ignore)
- handle concurrent clients (atomic appends) less hard than replaces

architecture
- **master node**: coordinate access w/ multiple **chunkservers** hosting 3 copies of data chunk/blocks
- master creates file handle for new files
- master:
	- returns replica locations of chunk wrt nodes 
	- file names --> chunks
	- coordinates migration, replication (if node dies)
- master tells client which replica to write to
- master advantages
	- simplier engineering
	- full picture of entire state
	- use complex alg for locating chunks instead of consistent masters
	- no bottleneck, only handles metadata
1. client converts file's byte offset --> chunk index (given chunk size)
2. requests file+chunk index to master
3. master gives client chunk handle and its replica locations
4. client requests specific byte range within chunk to repliac
5. chunkservers give data to client
[[c014d48fbfcd60719f0e76d40ff5bd59_MD5.jpeg|Open: Pasted image 20241122124117.png]]
![[c014d48fbfcd60719f0e76d40ff5bd59_MD5.jpeg]]
- ==Read-ahead==: master anticipate client may need later chunks and give handler
- if master dies? 
	- when master back online, loses all metadata
	- sends msg to all chunk servers
	- chunk servers respond with full index of chunks (heartbeat msgs)
	- so each write updates master (as client tells master to write)
	- periodic snapshots too? in case of corruption (cheaper + infrequent changes)
- chunk size?
	- larger chunks: less requests,